# -*- coding: utf-8 -*-
"""mxnet_16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v6MDMZFDoHpYp7OKAIjSHQYE-ZrSixeU
"""

!pip install mxnet gluoncv

from __future__ import print_function
import mxnet as mx
mx.random.seed(1)

data_ctx = mx.cpu(0)
model_ctx = mx.cpu(0)

# create syntethic data

num_inputs = 2
num_outputs = 1
num_examples = 10000

def real_fn(X):
    return 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2
    
X = mx.nd.random_normal(shape=(num_examples, num_inputs), ctx=data_ctx)
noise = .1 * mx.nd.random_normal(shape=(num_examples,), ctx=data_ctx)
y = real_fn(X) + noise

print(X.shape)
print(y.shape)

import matplotlib.pyplot as plt
plt.scatter(X[:, 1].asnumpy(),y.asnumpy())
plt.show()

plt.scatter(X[:, 0].asnumpy(),y.asnumpy())
plt.show()

# this samples data into batches for training.
batch_size = 4
train_data = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(X, y),
                                      batch_size=batch_size, shuffle=True)

for i, (data, label) in enumerate(train_data):
    print(data, label)
    break

# total batches
for i, (data, label) in enumerate(train_data):
    pass
print(i+1)

# create linear regression parameters
w = mx.nd.random_normal(shape=(num_inputs, num_outputs), ctx=model_ctx)
b = mx.nd.random_normal(shape=num_outputs, ctx=model_ctx)
params = [w, b]
# attach grads
for param in params:
    param.attach_grad()

def net(X):
    return mx.nd.dot(X, w) + b

def square_loss(yhat, y): 
    return mx.nd.mean((yhat - y) ** 2)

def SGD(params, lr):    
    for param in params:
        # the gradient of the each parameter in the model multiplied by the learning rate
        # affects the way the parameters are updated.
        param[:] = param - lr * param.grad

epochs = 10
learning_rate = .0001
num_batches = num_examples/batch_size

for e in range(epochs):
    cumulative_loss = 0
    # inner loop
    for i, (data, label) in enumerate(train_data):
        data = data.as_in_context(model_ctx)
        # change label shape from (4,) to (4, 1)
        label = label.as_in_context(model_ctx).reshape((-1, 1))
        with mx.autograd.record():
            output = net(data)
            loss = square_loss(output, label)
        loss.backward()
        SGD(params, learning_rate)
        cumulative_loss += loss.asscalar()
    print(cumulative_loss / num_batches)

############################################
#    Re-initialize parameters because they 
#    were already trained in the first loop
############################################
w[:] = mx.nd.random_normal(shape=(num_inputs, num_outputs), ctx=model_ctx)
b[:] = mx.nd.random_normal(shape=num_outputs, ctx=model_ctx)

############################################
#    Script to plot the losses over time
############################################
def plot(losses, X, sample_size=100):
    xs = list(range(len(losses)))
    f, (fg1, fg2) = plt.subplots(1, 2)
    fg1.set_title('Loss during training')
    fg1.plot(xs, losses, '-r')
    fg2.set_title('Estimated vs real function')
    fg2.plot(X[:sample_size, 1].asnumpy(),
             net(X[:sample_size, :]).asnumpy(), 'or', label='Estimated')
    fg2.plot(X[:sample_size, 1].asnumpy(),
             real_fn(X[:sample_size, :]).asnumpy(), '*g', label='Real')
    fg2.legend()

    plt.show()

learning_rate = .0001
losses = []
plot(losses, X)

for e in range(epochs):
    cumulative_loss = 0
    for i, (data, label) in enumerate(train_data):
        data = data.as_in_context(model_ctx)
        label = label.as_in_context(model_ctx).reshape((-1, 1))
        with mx.autograd.record():
            output = net(data)
            loss = square_loss(output, label)
        loss.backward()
        SGD(params, learning_rate)
        cumulative_loss += loss.asscalar()

    print("Epoch %s, batch %s. Mean loss: %s" % (e, i, cumulative_loss/num_batches))
    losses.append(cumulative_loss/num_batches)
            
plot(losses, X)